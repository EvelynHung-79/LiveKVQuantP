{
    "task": "qasper",
    "version": "v1",
    "args": {
        "input_mode": "longbench",
        "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "bench_version": "v1",
        "task_type": "single-doc",
        "num_samples": -1,
        "output_len": 64,
        "chunk_size": 512,
        "n_warmup": 2,
        "bits": 4,
        "ema_alpha": 0.2,
        "clip_factor_n": 4.0,
        "outlier_ratio": 0.01
    },
    "avg_score": 0.43337705870895477,
    "avg_latency_ms": 3648.955963534245,
    "max_peak_memory_mb": 18792.3134765625,
    "details": [
        {
            "index": 0,
            "score": 0.0,
            "metric": "F1",
            "output": "Manually by an expert, according to the characterization presented by Rubin et al.",
            "ground_truth": "Ground truth is not established in the paper",
            "peak_memory_mb": 17790.41015625,
            "latency_ms": 2340.3344469843432
        },
        {
            "index": 1,
            "score": 0.4117647058823529,
            "metric": "F1",
            "output": "An extension of the NetVLAD approach that adds Ghost clusters to map noisy or irrelevant content into ghost clusters and excludes their contribution during feature aggregation.",
            "ground_truth": "extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters",
            "peak_memory_mb": 15989.8056640625,
            "latency_ms": 2959.0705200098455
        },
        {
            "index": 2,
            "score": 0.0,
            "metric": "F1",
            "output": "68.8% to 71.8%",
            "ground_truth": "the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)",
            "peak_memory_mb": 16149.4775390625,
            "latency_ms": 2189.388506987598
        },
        {
            "index": 3,
            "score": 0.32258064516129037,
            "metric": "F1",
            "output": "Text sequences of context tweets are directly used as an additional feature of neural network models, and the use of context tweets is proposed to improve classification accuracy.",
            "ground_truth": "using tweets that one has replied or quoted to as contextual information",
            "peak_memory_mb": 15955.16162109375,
            "latency_ms": 2812.581702019088
        },
        {
            "index": 4,
            "score": 0.9090909090909091,
            "metric": "F1",
            "output": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney, and various other public pages.",
            "ground_truth": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney",
            "peak_memory_mb": 16149.47802734375,
            "latency_ms": 5452.9932489967905
        },
        {
            "index": 5,
            "score": 0.0,
            "metric": "F1",
            "output": "No.",
            "ground_truth": "Yes",
            "peak_memory_mb": 16309.6494140625,
            "latency_ms": 2074.084844032768
        },
        {
            "index": 6,
            "score": 0.0,
            "metric": "F1",
            "output": "Extrinsic evaluation based on the task of summary evaluation is proposed, where importance estimates are used to calculate scores for peer summaries.",
            "ground_truth": "Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2",
            "peak_memory_mb": 16390.7333984375,
            "latency_ms": 3970.590529963374
        },
        {
            "index": 7,
            "score": 0.7499999999999999,
            "metric": "F1",
            "output": "The CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum are used for evaluation.",
            "ground_truth": "CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum",
            "peak_memory_mb": 16470.697265625,
            "latency_ms": 4735.367159999441
        },
        {
            "index": 8,
            "score": 0.5,
            "metric": "F1",
            "output": "The proposed approach, GM_KL, achieves significantly better correlation scores than existing word sense disambiguation (WSD) approaches, such as w2g and w2gm, on various benchmark datasets.",
            "ground_truth": "GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.",
            "peak_memory_mb": 16031.40966796875,
            "latency_ms": 4069.781982980203
        },
        {
            "index": 9,
            "score": 0.16,
            "metric": "F1",
            "output": "Their ensemble method is a greedy method that selects models based on validation performance and then averages their predictions.",
            "ground_truth": "simply averaging the predictions from the constituent single models",
            "peak_memory_mb": 16309.64892578125,
            "latency_ms": 3345.601436041761
        },
        {
            "index": 10,
            "score": 0.625,
            "metric": "F1",
            "output": "The sources of the datasets are a TV sitcom (Friends) and Facebook messenger chats (EmotionPush).",
            "ground_truth": "Friends TV sitcom, Facebook messenger chats",
            "peak_memory_mb": 16229.5615234375,
            "latency_ms": 3049.6626929962076
        },
        {
            "index": 11,
            "score": 1.0,
            "metric": "F1",
            "output": "English.",
            "ground_truth": "English",
            "peak_memory_mb": 15989.80517578125,
            "latency_ms": 1146.5020769974217
        },
        {
            "index": 12,
            "score": 1.0,
            "metric": "F1",
            "output": "The IMDb dataset of movie reviews.",
            "ground_truth": "IMDb dataset of movie reviews",
            "peak_memory_mb": 15989.80615234375,
            "latency_ms": 1541.9588350341655
        },
        {
            "index": 13,
            "score": 0.47058823529411764,
            "metric": "F1",
            "output": "The proposed system achieves better performances than the baseline systems, with +1.08 F1 improvements on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ.",
            "ground_truth": "F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ",
            "peak_memory_mb": 16861.52490234375,
            "latency_ms": 7883.492049993947
        },
        {
            "index": 14,
            "score": 0.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "No",
            "peak_memory_mb": 16229.5625,
            "latency_ms": 1769.0394470118918
        },
        {
            "index": 15,
            "score": 0.4642857142857143,
            "metric": "F1",
            "output": "The Switchboard dataset, the Dialog State Tracking Challenge (DSTC), Twitter posts and news articles related to finance, a set of 124 questions collected from applying the Wizard of Oz method, a set of 184,001 Twitter posts and 62,949 news articles, and a set of 246,945 documents.",
            "ground_truth": "Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.",
            "peak_memory_mb": 18231.71142578125,
            "latency_ms": 21448.56114697177
        },
        {
            "index": 16,
            "score": 0.0,
            "metric": "F1",
            "output": "unanswerable",
            "ground_truth": "Energy with accuracy of 0.538",
            "peak_memory_mb": 17911.86767578125,
            "latency_ms": 9069.15132200811
        },
        {
            "index": 17,
            "score": 0.6666666666666665,
            "metric": "F1",
            "output": "RNN-based NMT, Transformer-NMT, and SMT (State-of-art Moses toolkit)",
            "ground_truth": "RNN-based NMT model, Transformer-NMT",
            "peak_memory_mb": 16505.70556640625,
            "latency_ms": 3284.3423620215617
        },
        {
            "index": 18,
            "score": 0.9130434782608696,
            "metric": "F1",
            "output": "Three regularization terms: (1) a regularization term associated with neutral features, (2) the maximum entropy of class distribution regularization term, and (3) the KL divergence between reference and predicted class distribution.",
            "ground_truth": "a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution",
            "peak_memory_mb": 16309.6474609375,
            "latency_ms": 5101.575047010556
        },
        {
            "index": 19,
            "score": 0.6136363636363635,
            "metric": "F1",
            "output": "1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and RCNN, 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, 7) UTCNN without the LDA model, 8) UTCNN without comments.",
            "ground_truth": "SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information",
            "peak_memory_mb": 16390.73583984375,
            "latency_ms": 11957.071490003727
        },
        {
            "index": 20,
            "score": 0.0,
            "metric": "F1",
            "output": "The system improves the state-of-the-art performance by several points.",
            "ground_truth": "They decrease MAE in 0.34",
            "peak_memory_mb": 16069.640625,
            "latency_ms": 2022.3402259871364
        },
        {
            "index": 21,
            "score": 0.41025641025641024,
            "metric": "F1",
            "output": "Their model improves interpretability by allowing attention heads to specialize more and with higher confidence, and by enabling sparsity, which facilitates the identification of head specializations.",
            "ground_truth": "the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence",
            "peak_memory_mb": 16712.203125,
            "latency_ms": 6048.808065010235
        },
        {
            "index": 22,
            "score": 0.5,
            "metric": "F1",
            "output": "The baseline model used was a Transformer base model.",
            "ground_truth": " MT system on the data released by BIBREF11",
            "peak_memory_mb": 16309.6474609375,
            "latency_ms": 2530.0294809858315
        },
        {
            "index": 23,
            "score": 0.4444444444444445,
            "metric": "F1",
            "output": "Labeled Attachment Scores (LAS) for zero-shot dependency parsing, accuracy for cross-lingual natural language inference (XNLI) test.",
            "ground_truth": "translation probabilities, Labeled Attachment Scores (LAS)",
            "peak_memory_mb": 16309.64892578125,
            "latency_ms": 4108.504818985239
        },
        {
            "index": 24,
            "score": 0.14285714285714288,
            "metric": "F1",
            "output": "MT (Machine Translation)",
            "ground_truth": "the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.",
            "peak_memory_mb": 16630.369140625,
            "latency_ms": 3404.9716239678673
        },
        {
            "index": 25,
            "score": 0.0,
            "metric": "F1",
            "output": "Emoticons, laughter expressions, and hashtags are among the stylistic features obtained.",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16229.56396484375,
            "latency_ms": 2882.797229976859
        },
        {
            "index": 26,
            "score": 0.6666666666666666,
            "metric": "F1",
            "output": "An LSTM (Long Short-Term Memory) encoder.",
            "ground_truth": "LSTM",
            "peak_memory_mb": 15989.80419921875,
            "latency_ms": 1635.596782027278
        },
        {
            "index": 27,
            "score": 1.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16950.3349609375,
            "latency_ms": 4158.1739480025135
        },
        {
            "index": 28,
            "score": 1.0,
            "metric": "F1",
            "output": "unanswerable",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16037.4716796875,
            "latency_ms": 1036.7571880342439
        },
        {
            "index": 29,
            "score": 1.0,
            "metric": "F1",
            "output": "22,880 users.",
            "ground_truth": "22,880 users",
            "peak_memory_mb": 16309.64892578125,
            "latency_ms": 2231.1699300189503
        },
        {
            "index": 30,
            "score": 0.21739130434782608,
            "metric": "F1",
            "output": "BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, BLEU-1, BLEU-4, recipe-level coherence, and recipe step entailment.",
            "ground_truth": "Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)",
            "peak_memory_mb": 16098.70361328125,
            "latency_ms": 4559.419767989311
        },
        {
            "index": 31,
            "score": 0.39999999999999997,
            "metric": "F1",
            "output": "They create labels for symptoms and attributes, such as time, activities that trigger symptoms, seriousness, frequency, and location.",
            "ground_truth": "(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer",
            "peak_memory_mb": 16229.56201171875,
            "latency_ms": 3326.640832994599
        },
        {
            "index": 32,
            "score": 0.0,
            "metric": "F1",
            "output": "unanswerable",
            "ground_truth": "57,505 sentences",
            "peak_memory_mb": 16390.73486328125,
            "latency_ms": 2340.3393320040777
        },
        {
            "index": 33,
            "score": 0.2222222222222222,
            "metric": "F1",
            "output": "Machine translation, specifically neural machine translation.",
            "ground_truth": "four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German",
            "peak_memory_mb": 16712.203125,
            "latency_ms": 3802.50218504807
        },
        {
            "index": 34,
            "score": 0.0,
            "metric": "F1",
            "output": "The article does not specify the improvement in performance for Estonian in the NER task.",
            "ground_truth": "5 percent points.",
            "peak_memory_mb": 16229.56201171875,
            "latency_ms": 2849.428360001184
        },
        {
            "index": 35,
            "score": 0.0,
            "metric": "F1",
            "output": "The authors have a background in the humanities and social sciences, and they come from diverse disciplinary backgrounds.",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 17110.509765625,
            "latency_ms": 6943.615928990766
        },
        {
            "index": 36,
            "score": 0.0,
            "metric": "F1",
            "output": "yes",
            "ground_truth": "No",
            "peak_memory_mb": 16149.79833984375,
            "latency_ms": 1283.13525399426
        },
        {
            "index": 37,
            "score": 0.6206896551724138,
            "metric": "F1",
            "output": "The Nguni languages (zul, xho, nbl, ssw) are similar to each other, and the Sotho languages (nso, sot, tsn) are also similar to each other.",
            "ground_truth": "Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)",
            "peak_memory_mb": 15909.96875,
            "latency_ms": 4057.557124993764
        },
        {
            "index": 38,
            "score": 0.08333333333333334,
            "metric": "F1",
            "output": "They compared their layer-wise trained models with models initialized with Xavier initialization algorithm.",
            "ground_truth": "Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers.",
            "peak_memory_mb": 16390.7333984375,
            "latency_ms": 3156.4046060084365
        },
        {
            "index": 39,
            "score": 0.2222222222222222,
            "metric": "F1",
            "output": "The Wikipedia dataset contains around 5K FA, 28K GA, 212K B, 533K C, 2.6M Start, and 3.2M Stub articles.",
            "ground_truth": "a sample of  29,794 wikipedia articles and 2,794 arXiv papers ",
            "peak_memory_mb": 16390.73388671875,
            "latency_ms": 5491.360599000473
        },
        {
            "index": 40,
            "score": 0.21621621621621623,
            "metric": "F1",
            "output": "A group of 50 native people who were well-versed in both English and Tamil languages acted as annotators for the evaluation.",
            "ground_truth": "50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.",
            "peak_memory_mb": 16550.53271484375,
            "latency_ms": 4858.446663012728
        },
        {
            "index": 41,
            "score": 1.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "Yes",
            "peak_memory_mb": 16390.73486328125,
            "latency_ms": 2294.007855001837
        },
        {
            "index": 42,
            "score": 0.163265306122449,
            "metric": "F1",
            "output": "Models are evaluated by measuring their efficiency (retention rate of tokens) and accuracy (fraction of sentences generated that exactly match the target sentence).",
            "ground_truth": "by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews",
            "peak_memory_mb": 15909.96875,
            "latency_ms": 2612.692844995763
        },
        {
            "index": 43,
            "score": 0.6153846153846153,
            "metric": "F1",
            "output": "Accuracy, Precision, Recall, F-measure, and ROUGE unigram f1 scores.",
            "ground_truth": "Precision, Recall, F-measure, accuracy",
            "peak_memory_mb": 16149.4765625,
            "latency_ms": 2681.44856399158
        },
        {
            "index": 44,
            "score": 0.16326530612244897,
            "metric": "F1",
            "output": "The source domain typically has sufficient labeled data, while the target domain has very few or no labeled data.",
            "ground_truth": "Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen",
            "peak_memory_mb": 16550.53125,
            "latency_ms": 4317.592900013551
        },
        {
            "index": 45,
            "score": 0.21428571428571427,
            "metric": "F1",
            "output": "LSTMs and several recent state-of-the-art methods, including RAN, QRNN, and NAS.",
            "ground_truth": "Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM",
            "peak_memory_mb": 16229.56201171875,
            "latency_ms": 3159.569636976812
        },
        {
            "index": 46,
            "score": 0.22222222222222224,
            "metric": "F1",
            "output": "NeuronBlocks supports Word/character embedding, RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, Linear/Bi-linear Attention, Full Attention, Bidirectional attention flow, Dropout, Layer Norm, Batch Norm, and Focal Loss.",
            "ground_truth": "Embedding Layer, Neural Network Layers, Loss Function, Metrics",
            "peak_memory_mb": 15882.4609375,
            "latency_ms": 4401.399449037854
        },
        {
            "index": 47,
            "score": 0.8571428571428571,
            "metric": "F1",
            "output": "The Carnegie Mellon Pronouncing Dictionary and the multilingual pronunciation corpus collected by deri2016grapheme from Wiktionary.",
            "ground_truth": "the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary",
            "peak_memory_mb": 16149.4775390625,
            "latency_ms": 3201.157601957675
        },
        {
            "index": 48,
            "score": 0.0,
            "metric": "F1",
            "output": "There is no mention of baselines in the article.",
            "ground_truth": "varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)",
            "peak_memory_mb": 15989.80615234375,
            "latency_ms": 1715.8680959837511
        },
        {
            "index": 49,
            "score": 0.6,
            "metric": "F1",
            "output": "English, Spanish, Finnish, and 13 other languages.",
            "ground_truth": "English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish",
            "peak_memory_mb": 16370.36083984375,
            "latency_ms": 2835.3375180158764
        },
        {
            "index": 50,
            "score": 0.0,
            "metric": "F1",
            "output": "Named Entity Recognition, POS tagging, text classification, and language modeling.",
            "ground_truth": "None",
            "peak_memory_mb": 16069.6396484375,
            "latency_ms": 2033.8327859644778
        },
        {
            "index": 51,
            "score": 1.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "Yes",
            "peak_memory_mb": 16390.7353515625,
            "latency_ms": 2317.8372950060293
        },
        {
            "index": 52,
            "score": 0.0,
            "metric": "F1",
            "output": "Yes, empirical evaluations on the response retrieval task using Reddit, OpenSubtitles, and AmazonQA conversational test data showed strong and robust performance.",
            "ground_truth": "No",
            "peak_memory_mb": 16069.64208984375,
            "latency_ms": 3261.6489159991033
        },
        {
            "index": 53,
            "score": 0.0,
            "metric": "F1",
            "output": "By analyzing blog posts for psycholinguistic and semantic properties, such as word categories and core values.",
            "ground_truth": "using the Meaning Extraction Method",
            "peak_memory_mb": 15828.86181640625,
            "latency_ms": 1718.9509950112551
        },
        {
            "index": 54,
            "score": 1.0,
            "metric": "F1",
            "output": "Claim, premise, backing, rebuttal, and refutation.",
            "ground_truth": "claim, premise, backing, rebuttal, and refutation",
            "peak_memory_mb": 18552.05419921875,
            "latency_ms": 14610.061658953782
        },
        {
            "index": 55,
            "score": 0.0,
            "metric": "F1",
            "output": "ngrams of order INLINEFORM7 (n-grams)",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16804.193359375,
            "latency_ms": 2679.109189019073
        },
        {
            "index": 56,
            "score": 0.8571428571428571,
            "metric": "F1",
            "output": "Roughly 14k tweets, from 1,873 conversation threads.",
            "ground_truth": "1,873 Twitter conversation threads, roughly 14k tweets",
            "peak_memory_mb": 16229.56298828125,
            "latency_ms": 2770.8538210135885
        },
        {
            "index": 57,
            "score": 0.896551724137931,
            "metric": "F1",
            "output": "English, Chinese Mandarin, French, German, Russian, Spanish, Estonian, Finnish, Polish, Welsh, Kiswahili, and Yue Chinese.",
            "ground_truth": "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese",
            "peak_memory_mb": 18792.3134765625,
            "latency_ms": 19868.175380048342
        },
        {
            "index": 58,
            "score": 0.631578947368421,
            "metric": "F1",
            "output": "The two datasets are the \"Conversations Gone Awry\" dataset and the CMV dataset from the subreddit ChangeMyView.",
            "ground_truth": " `Conversations Gone Awry' dataset, subreddit ChangeMyView",
            "peak_memory_mb": 16940.5068359375,
            "latency_ms": 4524.227640999015
        },
        {
            "index": 59,
            "score": 0.0,
            "metric": "F1",
            "output": "unanswerable",
            "ground_truth": "No",
            "peak_memory_mb": 15989.80419921875,
            "latency_ms": 1171.6125620296225
        },
        {
            "index": 60,
            "score": 0.4912280701754386,
            "metric": "F1",
            "output": "We applied various sanity checks to the translations, including computing sentence-level BLEU scores, manually inspecting examples, measuring perplexity, and computing similarity scores between transcripts and translations.",
            "ground_truth": "Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets",
            "peak_memory_mb": 16069.64013671875,
            "latency_ms": 3633.2091049989685
        },
        {
            "index": 61,
            "score": 0.611111111111111,
            "metric": "F1",
            "output": "They combine audio and text sequences via dual RNNs and then through a feed-forward neural model.",
            "ground_truth": "combines the information from these sources using a feed-forward neural model",
            "peak_memory_mb": 16149.4775390625,
            "latency_ms": 2838.998867024202
        },
        {
            "index": 62,
            "score": 0.5365853658536586,
            "metric": "F1",
            "output": "Our method (NMT+synthetic) yields an improvement over our baseline NMT by 2.11 BLEU, 1.7 FKGL and 1.07 SARI.",
            "ground_truth": "For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.",
            "peak_memory_mb": 15989.80517578125,
            "latency_ms": 3714.814388018567
        },
        {
            "index": 63,
            "score": 0.0,
            "metric": "F1",
            "output": "about 7 humans participated in the human evaluation.",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16309.64794921875,
            "latency_ms": 2536.5968830301426
        },
        {
            "index": 64,
            "score": 0.5384615384615385,
            "metric": "F1",
            "output": "Tweets were considered to have gone viral if they were retweeted more than 1000 times.",
            "ground_truth": "Viral tweets are the ones that are retweeted more than 1000 times",
            "peak_memory_mb": 16107.54638671875,
            "latency_ms": 2621.344004990533
        },
        {
            "index": 65,
            "score": 1.0,
            "metric": "F1",
            "output": "BERT",
            "ground_truth": "BERT",
            "peak_memory_mb": 15909.9677734375,
            "latency_ms": 858.0847069970332
        },
        {
            "index": 66,
            "score": 0.5714285714285715,
            "metric": "F1",
            "output": "crowdsourcing, specifically using an Android application.",
            "ground_truth": "Android application",
            "peak_memory_mb": 16309.6474609375,
            "latency_ms": 2583.3713589818217
        },
        {
            "index": 67,
            "score": 0.22222222222222224,
            "metric": "F1",
            "output": "Logistic Regression and a Deep Learning Model with GloVe word embeddings are used for RQE.",
            "ground_truth": "Logistic Regression, neural networks",
            "peak_memory_mb": 17030.4228515625,
            "latency_ms": 6334.741846018005
        },
        {
            "index": 68,
            "score": 0.30303030303030304,
            "metric": "F1",
            "output": "The benchmark dataset used is the Social Honeypot dataset, which was created by Lee et al. and contains 19,276 legitimate users and 22,223 spammers, collected over 7 months.",
            "ground_truth": "Social Honeypot dataset (public) and Weibo dataset (self-collected); yes",
            "peak_memory_mb": 16101.64990234375,
            "latency_ms": 4367.701039998792
        },
        {
            "index": 69,
            "score": 0.11764705882352941,
            "metric": "F1",
            "output": "The decoder is an LSTM that generates characters in the output word form using encoder states and an attention mechanism.",
            "ground_truth": "LSTM",
            "peak_memory_mb": 15989.80419921875,
            "latency_ms": 2292.341694002971
        },
        {
            "index": 70,
            "score": 0.0,
            "metric": "F1",
            "output": "yes",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16309.650390625,
            "latency_ms": 2056.2790569965728
        },
        {
            "index": 71,
            "score": 0.43333333333333335,
            "metric": "F1",
            "output": "For SLC task, the best performing model is the ensemble of Logistic Regression, CNN and BERT (r19) with an F1 score of 0.673. For FLC task, the best performing model is the ensemble of Multi-grain LSTM-CRF with BERT (ensemble+) with a F1 score on the test set not explicitly mentioned.",
            "ground_truth": "For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).",
            "peak_memory_mb": 15909.9677734375,
            "latency_ms": 6527.894105995074
        },
        {
            "index": 72,
            "score": 0.09090909090909091,
            "metric": "F1",
            "output": "A strong baseline established with monolingual data (#10 in Table TABREF33) and a weak baseline without using any monolingual data (b3 in Table TABREF27).",
            "ground_truth": "pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17",
            "peak_memory_mb": 16497.75927734375,
            "latency_ms": 5653.044785023667
        },
        {
            "index": 73,
            "score": 1.0,
            "metric": "F1",
            "output": "0.7033 (for List-type questions)",
            "ground_truth": "0.7033",
            "peak_memory_mb": 16973.18212890625,
            "latency_ms": 5161.563632020261
        },
        {
            "index": 74,
            "score": 0.3333333333333333,
            "metric": "F1",
            "output": "word2vec, Skip-gram, and CBOW.",
            "ground_truth": "Skipâ€“gram, CBOW",
            "peak_memory_mb": 16390.734375,
            "latency_ms": 2968.486033962108
        },
        {
            "index": 75,
            "score": 0.23529411764705882,
            "metric": "F1",
            "output": "They use generic and Hindi-tuned reordering rules, and also two re-ordering systems from the CFILT-preorder system.",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 15989.8037109375,
            "latency_ms": 2690.897644031793
        },
        {
            "index": 76,
            "score": 0.0,
            "metric": "F1",
            "output": "unanswerable",
            "ground_truth": "Yes",
            "peak_memory_mb": 16116.0478515625,
            "latency_ms": 1504.1652789805084
        },
        {
            "index": 77,
            "score": 0.375,
            "metric": "F1",
            "output": "Seven experts with legal training and anonymous reviewers were used for annotation.",
            "ground_truth": "Individuals with legal training",
            "peak_memory_mb": 16309.6484375,
            "latency_ms": 2770.523543993477
        },
        {
            "index": 78,
            "score": 0.11764705882352941,
            "metric": "F1",
            "output": "An actor-critic architecture is used for painting embedding, and a sequence-to-sequence model with attention is used for language style transfer.",
            "ground_truth": "generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models",
            "peak_memory_mb": 15829.13330078125,
            "latency_ms": 2252.616957994178
        },
        {
            "index": 79,
            "score": 0.23529411764705882,
            "metric": "F1",
            "output": "The Transformer layer works better, as evidenced by the fact that ToBERT outperforms RoBERT in most cases.",
            "ground_truth": "Transformer over BERT (ToBERT)",
            "peak_memory_mb": 16069.64111328125,
            "latency_ms": 2733.5879409802146
        },
        {
            "index": 80,
            "score": 1.0,
            "metric": "F1",
            "output": "yes",
            "ground_truth": "Yes",
            "peak_memory_mb": 16390.734375,
            "latency_ms": 2167.3657979699783
        },
        {
            "index": 81,
            "score": 1.0,
            "metric": "F1",
            "output": "personal attack, racism, and sexism.",
            "ground_truth": "personal attack, racism, and sexism",
            "peak_memory_mb": 16149.4775390625,
            "latency_ms": 1984.4915670109913
        },
        {
            "index": 82,
            "score": 0.7058823529411765,
            "metric": "F1",
            "output": "They split the context into three regions (left, middle, and right) and use two contexts: a combination of the left context, the left entity, and the middle context; and a combination of the middle context, the right entity, and the right context.",
            "ground_truth": "They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.",
            "peak_memory_mb": 15989.80615234375,
            "latency_ms": 5019.403936050367
        },
        {
            "index": 83,
            "score": 0.14285714285714285,
            "metric": "F1",
            "output": "There are four major classes of entities in the dataset: PER (Person), LOC (Location), ORG (Organization) and MISC (Miscellaneous).",
            "ground_truth": "OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities",
            "peak_memory_mb": 16149.47705078125,
            "latency_ms": 3662.9686360247433
        },
        {
            "index": 84,
            "score": 0.15384615384615383,
            "metric": "F1",
            "output": "The exact amount of improvement in quality is not specified in the article.",
            "ground_truth": "improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added",
            "peak_memory_mb": 16390.734375,
            "latency_ms": 3146.897485014051
        },
        {
            "index": 85,
            "score": 0.22222222222222218,
            "metric": "F1",
            "output": "The imbalance between men and women in the analyzed corpora is significant, with 65% of speakers being men, speaking more than 75% of the time.",
            "ground_truth": "Women represent 33.16% of the speakers",
            "peak_memory_mb": 16309.6494140625,
            "latency_ms": 4492.070560983848
        },
        {
            "index": 86,
            "score": 0.5,
            "metric": "F1",
            "output": "English-German dataset, as compared to BIBREF30.",
            "ground_truth": "the English-German dataset",
            "peak_memory_mb": 15909.9677734375,
            "latency_ms": 1524.71007197164
        },
        {
            "index": 87,
            "score": 0.0,
            "metric": "F1",
            "output": "BIBREF18, BIBREF20.",
            "ground_truth": "Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019",
            "peak_memory_mb": 16309.64794921875,
            "latency_ms": 2520.5449799541384
        },
        {
            "index": 88,
            "score": 0.5714285714285715,
            "metric": "F1",
            "output": "Logistic Regression and Multilayer Perceptron (MLP) are used as the target models.",
            "ground_truth": "probabilistic model",
            "peak_memory_mb": 16550.53076171875,
            "latency_ms": 4352.336433017626
        },
        {
            "index": 89,
            "score": 0.5925925925925927,
            "metric": "F1",
            "output": "NLTK, Stanford CoreNLP, TwitterNLP, SentiStrength, TensiStrength, BIBREF17, BIBREF18, CogComp-NLP, spaCy, and Stanford NLP NER.",
            "ground_truth": "BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21",
            "peak_memory_mb": 15859.56689453125,
            "latency_ms": 3699.4923299644142
        },
        {
            "index": 90,
            "score": 0.6666666666666666,
            "metric": "F1",
            "output": "The SQuAD dataset.",
            "ground_truth": "SQuAD",
            "peak_memory_mb": 16390.7333984375,
            "latency_ms": 2486.048280959949
        },
        {
            "index": 91,
            "score": 0.0,
            "metric": "F1",
            "output": "Various existing approaches have been proposed for modelling urban regions, identifying points-of-interest, and itineraries, but the usefulness of Flickr for characterizing the natural environment is less well-understood.",
            "ground_truth": "BOW-Tags, BOW-KL(Tags), BOW-All, GloVe",
            "peak_memory_mb": 16470.69482421875,
            "latency_ms": 5531.083518988453
        },
        {
            "index": 92,
            "score": 1.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "Yes",
            "peak_memory_mb": 15909.9677734375,
            "latency_ms": 911.600798019208
        },
        {
            "index": 93,
            "score": 0.4444444444444445,
            "metric": "F1",
            "output": "They used CSAT, 20 newsgroups, and Fisher datasets for evaluation.",
            "ground_truth": "CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus",
            "peak_memory_mb": 16069.64111328125,
            "latency_ms": 2297.9342849575914
        },
        {
            "index": 94,
            "score": 0.888888888888889,
            "metric": "F1",
            "output": "The IMDb movie review dataset.",
            "ground_truth": "the IMDb movie review dataset BIBREF17",
            "peak_memory_mb": 16229.5625,
            "latency_ms": 2031.745955988299
        },
        {
            "index": 95,
            "score": 1.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "Yes",
            "peak_memory_mb": 15829.13232421875,
            "latency_ms": 737.5617640209384
        },
        {
            "index": 96,
            "score": 0.0,
            "metric": "F1",
            "output": "unanswerable",
            "ground_truth": "No",
            "peak_memory_mb": 15829.13330078125,
            "latency_ms": 822.0908929943107
        },
        {
            "index": 97,
            "score": 0.4,
            "metric": "F1",
            "output": "The invertibility condition is that INLINEFORM0 and INLINEFORM1 exists.",
            "ground_truth": "The neural projector must be invertible.",
            "peak_memory_mb": 16390.7353515625,
            "latency_ms": 3290.298444975633
        },
        {
            "index": 98,
            "score": 0.0909090909090909,
            "metric": "F1",
            "output": "The proposed qualitative annotation schema is a multi-label task that includes dimensions of interest such as factual correctness, required reasoning, and linguistic complexity, with subcategories including factual knowledge, intuitive knowledge, lexical semantics, syntax, and complexity of reasoning.",
            "ground_truth": "The resulting taxonomy of the framework is shown in Figure FIGREF10",
            "peak_memory_mb": 16520.42529296875,
            "latency_ms": 6690.107628004625
        },
        {
            "index": 99,
            "score": 0.5454545454545454,
            "metric": "F1",
            "output": "WikiSmall: 89,042 sentence pairs (training) and 100 pairs (test); WikiLarge: 296,402 sentence pairs (training).",
            "ground_truth": "training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing",
            "peak_memory_mb": 15989.80517578125,
            "latency_ms": 3072.4605669965968
        },
        {
            "index": 100,
            "score": 1.0,
            "metric": "F1",
            "output": "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train.",
            "ground_truth": "Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation",
            "peak_memory_mb": 16630.36865234375,
            "latency_ms": 5442.444066051394
        },
        {
            "index": 101,
            "score": 1.0,
            "metric": "F1",
            "output": "English.",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16390.7353515625,
            "latency_ms": 2319.985946000088
        },
        {
            "index": 102,
            "score": 0.6956521739130436,
            "metric": "F1",
            "output": "A linear SVM, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model are used in the experiment.",
            "ground_truth": "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)",
            "peak_memory_mb": 15989.80419921875,
            "latency_ms": 3564.3629879923537
        },
        {
            "index": 103,
            "score": 0.0,
            "metric": "F1",
            "output": "unanswerable",
            "ground_truth": "No",
            "peak_memory_mb": 15829.13232421875,
            "latency_ms": 791.164188995026
        },
        {
            "index": 104,
            "score": 0.7692307692307692,
            "metric": "F1",
            "output": "GloVe, Edinburgh embeddings, and Emoji embeddings.",
            "ground_truth": "Pretrained word embeddings  were not used",
            "peak_memory_mb": 15883.90283203125,
            "latency_ms": 1324.7001499985345
        },
        {
            "index": 105,
            "score": 0.13333333333333333,
            "metric": "F1",
            "output": "All personalized models outperformed the baseline in BPE perplexity, while Prior Name performed the best.",
            "ground_truth": "average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time",
            "peak_memory_mb": 16100.02783203125,
            "latency_ms": 2681.867247039918
        },
        {
            "index": 106,
            "score": 0.3,
            "metric": "F1",
            "output": "The combination of rewards for reinforcement learning is the harmonic mean of the irony reward and the sentiment reward.",
            "ground_truth": "irony accuracy, sentiment preservation",
            "peak_memory_mb": 16454.53466796875,
            "latency_ms": 3750.1031500287354
        },
        {
            "index": 107,
            "score": 0.4888888888888889,
            "metric": "F1",
            "output": "The authors demonstrate that their model may not work well with Shakespeare style transfer when the style transfer dataset does not have similar words in the training set of sentences.",
            "ground_truth": "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer",
            "peak_memory_mb": 15829.13330078125,
            "latency_ms": 2538.6110059916973
        },
        {
            "index": 108,
            "score": 0.9411764705882353,
            "metric": "F1",
            "output": "The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.",
            "ground_truth": "Affective Text, Fairy Tales, ISEAR",
            "peak_memory_mb": 16149.47802734375,
            "latency_ms": 2706.848789996002
        },
        {
            "index": 109,
            "score": 0.4000000000000001,
            "metric": "F1",
            "output": "Their distribution results showed significant differences between viral tweets containing fake news and viral tweets not containing fake news in followers, URLs, and verification of the users.",
            "ground_truth": "Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different",
            "peak_memory_mb": 16105.58740234375,
            "latency_ms": 3368.5584259801544
        },
        {
            "index": 110,
            "score": 0.5853658536585366,
            "metric": "F1",
            "output": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset and is curated from 1,268 randomly selected tweets.",
            "ground_truth": "1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset",
            "peak_memory_mb": 16309.6494140625,
            "latency_ms": 3799.8098710086197
        },
        {
            "index": 111,
            "score": 1.0,
            "metric": "F1",
            "output": "unanswerable",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16309.6474609375,
            "latency_ms": 2031.971410033293
        },
        {
            "index": 112,
            "score": 0.28571428571428564,
            "metric": "F1",
            "output": "a compact, scalable and meaningful representation of the whole set of word vectors.",
            "ground_truth": "Word vectors, usually in the context of others within the same class",
            "peak_memory_mb": 16712.20361328125,
            "latency_ms": 4439.770512050018
        },
        {
            "index": 113,
            "score": 0.19672131147540986,
            "metric": "F1",
            "output": "Baseline model B1 uses only the salience-based features.",
            "ground_truth": "For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.",
            "peak_memory_mb": 17110.5068359375,
            "latency_ms": 5808.059863978997
        },
        {
            "index": 114,
            "score": 1.0,
            "metric": "F1",
            "output": "unanswerable",
            "ground_truth": "Yes",
            "peak_memory_mb": 16114.87939453125,
            "latency_ms": 1164.230942958966
        },
        {
            "index": 115,
            "score": 1.0,
            "metric": "F1",
            "output": "unanswerable",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16069.64013671875,
            "latency_ms": 1373.642867023591
        },
        {
            "index": 116,
            "score": 0.6428571428571429,
            "metric": "F1",
            "output": "They used the high-quality datasets from the SemEval-2016 â€œSentiment Analysis in Twitterâ€ task, specifically the dataset for ternary and fine-grained sentiment classification.",
            "ground_truth": " high-quality datasets  from SemEval-2016 â€œSentiment Analysis in Twitterâ€ task",
            "peak_memory_mb": 16069.640625,
            "latency_ms": 3729.830909986049
        },
        {
            "index": 117,
            "score": 0.0,
            "metric": "F1",
            "output": "They use the pre-trained uncased BERT$_\\mathrm {BASE}$ model.",
            "ground_truth": "small BERT",
            "peak_memory_mb": 15989.8037109375,
            "latency_ms": 2013.089795014821
        },
        {
            "index": 118,
            "score": 0.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "No",
            "peak_memory_mb": 16950.3349609375,
            "latency_ms": 4178.537516971119
        },
        {
            "index": 119,
            "score": 0.0,
            "metric": "F1",
            "output": "The images are from a controlled data generation framework consisting of abstract colored shapes.",
            "ground_truth": "Yes",
            "peak_memory_mb": 16229.56201171875,
            "latency_ms": 2636.5515899960883
        },
        {
            "index": 120,
            "score": 0.14634146341463414,
            "metric": "F1",
            "output": "Our model's performance is competitive or even state-of-the-art for some of the emotion labels on existing, standard evaluation datasets.",
            "ground_truth": "Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. ",
            "peak_memory_mb": 16149.47802734375,
            "latency_ms": 3248.443780001253
        },
        {
            "index": 121,
            "score": 0.32142857142857145,
            "metric": "F1",
            "output": "We first design a simple tagging scheme consisting of two tags {IN: Not a pun, OUT: Pun}. Then we introduce a new tagging scheme consisting of three tags, namely {B: Before, I: Inside, E: After}, where I tag indicates that the current word is a pun.",
            "ground_truth": "A new tagging scheme that tags the words before and after the pun as well as the pun words.",
            "peak_memory_mb": 16069.64208984375,
            "latency_ms": 6100.95532995183
        },
        {
            "index": 122,
            "score": 1.0,
            "metric": "F1",
            "output": "no",
            "ground_truth": "No",
            "peak_memory_mb": 16069.64013671875,
            "latency_ms": 1259.3053249875084
        },
        {
            "index": 123,
            "score": 0.34782608695652173,
            "metric": "F1",
            "output": "The robustness of a model is defined as its ability to handle bias in prior knowledge and not be misled by unbalanced labeled features.",
            "ground_truth": "ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced",
            "peak_memory_mb": 16309.6474609375,
            "latency_ms": 3838.9596499619074
        },
        {
            "index": 124,
            "score": 0.5217391304347825,
            "metric": "F1",
            "output": "The article evaluates the following sentence embeddings methods: InferSent, Universal Sentence Encoder, Skip-Thought, GloVe, RoBERTa, BERT (average embeddings and CLS-token output), and poly-encoders.",
            "ground_truth": "GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent",
            "peak_memory_mb": 16390.7333984375,
            "latency_ms": 5758.916539954953
        },
        {
            "index": 125,
            "score": 0.7586206896551724,
            "metric": "F1",
            "output": "Our proposed method outperforms BERT-MRC by +0.29 on English CoNLL2003 and +0.96 on English OntoNotes5.0, achieving F1 improvements by +0.97 on Chinese MSRA and +2.36 on Chinese OntoNotes4.0.",
            "ground_truth": "English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively",
            "peak_memory_mb": 16390.7333984375,
            "latency_ms": 7899.073813983705
        },
        {
            "index": 126,
            "score": 0.9600000000000001,
            "metric": "F1",
            "output": "Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.",
            "ground_truth": "Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions",
            "peak_memory_mb": 15989.8056640625,
            "latency_ms": 2043.5823230072856
        },
        {
            "index": 127,
            "score": 0.32558139534883723,
            "metric": "F1",
            "output": "Their model, SATA Tree-LSTM, showed superior or competitive performance compared to previous syntactic tree-based models as well as other neural models including Latent Syntax Tree-LSTM, Tree-based CNN, Gumbel Tree-LSTM, NSE, Reinforced Self-Attention Network, and Residual stacked encoders.",
            "ground_truth": "Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks",
            "peak_memory_mb": 16790.1630859375,
            "latency_ms": 10047.019880032167
        },
        {
            "index": 128,
            "score": 0.0,
            "metric": "F1",
            "output": "Relation detection",
            "ground_truth": "answer questions by obtaining information from KB tuples ",
            "peak_memory_mb": 16550.53125,
            "latency_ms": 2792.15665202355
        },
        {
            "index": 129,
            "score": 0.888888888888889,
            "metric": "F1",
            "output": "A name-based Nearest-Neighbor model (NN) and a simple Encoder-Decoder baseline with ingredient attention (Enc-Dec).",
            "ground_truth": "name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)",
            "peak_memory_mb": 16098.0400390625,
            "latency_ms": 2973.492884018924
        },
        {
            "index": 130,
            "score": 0.2040816326530612,
            "metric": "F1",
            "output": "A browser-based annotation tool, manually categorizing images, and leveraging the structure of Flickr30K Entities by applying Louvain clustering to the coreference graph are considered methods to find examples of biases and unwarranted inferences.",
            "ground_truth": "spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering",
            "peak_memory_mb": 15945.88330078125,
            "latency_ms": 4099.808238970581
        },
        {
            "index": 131,
            "score": 0.7142857142857143,
            "metric": "F1",
            "output": "French, Italian, Spanish, English, German, and Hebrew.",
            "ground_truth": "English, French, German ",
            "peak_memory_mb": 15989.80615234375,
            "latency_ms": 1850.3586709848605
        },
        {
            "index": 132,
            "score": 0.125,
            "metric": "F1",
            "output": "They experimented with the Cell-aware Stacked LSTM (CAS-LSTM) model and its variants, as well as a bidirectional CAS-LSTM (Bi-CAS-LSTM) network, including models with and without peephole connections.",
            "ground_truth": "Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers",
            "peak_memory_mb": 16202.66064453125,
            "latency_ms": 5203.793497988954
        },
        {
            "index": 133,
            "score": 1.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "Yes",
            "peak_memory_mb": 16712.205078125,
            "latency_ms": 3423.0778100318275
        },
        {
            "index": 134,
            "score": 0.10526315789473684,
            "metric": "F1",
            "output": "The authors experimented with several summarization algorithms provided by the Sumy package, including ILP-based summarization.",
            "ground_truth": "LSA, TextRank, LexRank and ILP-based summary.",
            "peak_memory_mb": 16149.4765625,
            "latency_ms": 2903.2995859743096
        },
        {
            "index": 135,
            "score": 0.0,
            "metric": "F1",
            "output": "Probabilistic graphical models proposed by BIBREF0.",
            "ground_truth": "hLSTM",
            "peak_memory_mb": 16229.5634765625,
            "latency_ms": 2515.6375159858726
        },
        {
            "index": 136,
            "score": 0.06060606060606061,
            "metric": "F1",
            "output": "The identity function used in the \"Neighbors-only\" ablation study is the least impactful.",
            "ground_truth": "Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets.",
            "peak_memory_mb": 16470.6953125,
            "latency_ms": 3814.160021021962
        },
        {
            "index": 137,
            "score": 0.23529411764705882,
            "metric": "F1",
            "output": "The two corpora used in the shared task correspond to the diachronic corpus pair from DTA18 and DTA19.",
            "ground_truth": "DTA18, DTA19",
            "peak_memory_mb": 15909.96875,
            "latency_ms": 2417.3549440456554
        },
        {
            "index": 138,
            "score": 0.9333333333333333,
            "metric": "F1",
            "output": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.",
            "ground_truth": "Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam",
            "peak_memory_mb": 15989.8056640625,
            "latency_ms": 2168.496188998688
        },
        {
            "index": 139,
            "score": 0.09999999999999999,
            "metric": "F1",
            "output": "The model achieves competitive performance on the target language reading comprehension task, even surpassing human-level performance in some cases.",
            "ground_truth": "Table TABREF6, Table TABREF8",
            "peak_memory_mb": 16044.8017578125,
            "latency_ms": 2576.663510990329
        },
        {
            "index": 140,
            "score": 0.15384615384615383,
            "metric": "F1",
            "output": "ALOHA, combined with the HLAs and dialogue dataset, achieves a significant improvement on the target character language style retrieval task compared to the baseline open-domain chatbot models.",
            "ground_truth": "Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)",
            "peak_memory_mb": 16630.3662109375,
            "latency_ms": 5958.35554698715
        },
        {
            "index": 141,
            "score": 0.08571428571428572,
            "metric": "F1",
            "output": "Our model performs better than several state-of-the-art GAN baselines with lower training variance.",
            "ground_truth": "ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.",
            "peak_memory_mb": 16309.6484375,
            "latency_ms": 3246.2926709558815
        },
        {
            "index": 142,
            "score": 0.19999999999999998,
            "metric": "F1",
            "output": "The authors present evidence of the model's ability to detect biases through manual inspection of mislabeled items, examination of tweets containing implicit abuse, and analysis of datasets with specific biases in data collection and annotation.",
            "ground_truth": "The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate",
            "peak_memory_mb": 16390.734375,
            "latency_ms": 5521.484387980308
        },
        {
            "index": 143,
            "score": 0.5,
            "metric": "F1",
            "output": "Yes, the article mentions that several baselines, including SVM, CNN, No-Answer Baseline (NA), Word Count Baseline, and human performance, were tested to compare with the neural baseline.",
            "ground_truth": "SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance",
            "peak_memory_mb": 16309.6484375,
            "latency_ms": 5178.347503009718
        },
        {
            "index": 144,
            "score": 0.16666666666666666,
            "metric": "F1",
            "output": "The OurNepali dataset has a volume almost ten times bigger compared to ILPRL dataset in terms of entities.",
            "ground_truth": "Dataset contains 3606 total sentences and 79087 total entities.",
            "peak_memory_mb": 16149.47705078125,
            "latency_ms": 3110.4811539989896
        },
        {
            "index": 145,
            "score": 1.0,
            "metric": "F1",
            "output": "Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP.",
            "ground_truth": "Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP",
            "peak_memory_mb": 16390.7333984375,
            "latency_ms": 4109.139978012536
        },
        {
            "index": 146,
            "score": 0.2608695652173913,
            "metric": "F1",
            "output": "The datasets used include ERP data from BIBREF0, eye-tracking data, self-paced reading time, and a chapter of Harry Potter and the Sorcerer's Stone.",
            "ground_truth": "Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)",
            "peak_memory_mb": 15877.42431640625,
            "latency_ms": 2843.52667798521
        },
        {
            "index": 147,
            "score": 0.7692307692307693,
            "metric": "F1",
            "output": "Multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic and 4 words.",
            "ground_truth": "7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)",
            "peak_memory_mb": 15989.8056640625,
            "latency_ms": 2798.8887740066275
        },
        {
            "index": 148,
            "score": 0.7058823529411764,
            "metric": "F1",
            "output": "Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN, Pointer-Gen+ARL-SEN, and test set headlines.",
            "ground_truth": "Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN",
            "peak_memory_mb": 16470.69580078125,
            "latency_ms": 7659.359320008662
        },
        {
            "index": 149,
            "score": 0.14814814814814814,
            "metric": "F1",
            "output": "Traditional machine learning models (NaÃ¯ve Bayes, Logistic Regression, SVM, Random Forests, Gradient Boosted Trees) and neural network based models (Convolutional Neural Networks, Recurrent Neural Networks and their variants).",
            "ground_truth": "NaÃ¯ve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)",
            "peak_memory_mb": 15955.71142578125,
            "latency_ms": 4020.9089849959128
        },
        {
            "index": 150,
            "score": 0.4,
            "metric": "F1",
            "output": "A bi-directional language model and a uni-directional model, both using self-attention, are considered.",
            "ground_truth": "uni-directional model to augment the decoder",
            "peak_memory_mb": 15909.96923828125,
            "latency_ms": 2155.6363780400716
        },
        {
            "index": 151,
            "score": 0.6875,
            "metric": "F1",
            "output": "They are dynamically adjusted in proportion to $(1-p)$, and this weight dynamically changes as training proceeds.",
            "ground_truth": "One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.",
            "peak_memory_mb": 16385.39111328125,
            "latency_ms": 3559.0272059780546
        },
        {
            "index": 152,
            "score": 0.4347826086956522,
            "metric": "F1",
            "output": "Our methods successfully pass the bottleneck and achieve a comparable score within a margin of error.",
            "ground_truth": "Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.",
            "peak_memory_mb": 15989.8056640625,
            "latency_ms": 2082.481261983048
        },
        {
            "index": 153,
            "score": 0.15384615384615383,
            "metric": "F1",
            "output": "A Bayesian model for each language, and crosslingual latent variables to incorporate soft role agreement between aligned constituents.",
            "ground_truth": "Bayesian model of garg2012unsupervised as our base monolingual model",
            "peak_memory_mb": 16309.6484375,
            "latency_ms": 3488.2131419726647
        },
        {
            "index": 154,
            "score": 0.0,
            "metric": "F1",
            "output": "Through the use of annotations for noises and disfluencies, including aborted words, mispronunciations, poor intelligibility.",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16149.4765625,
            "latency_ms": 3204.4165650149807
        },
        {
            "index": 155,
            "score": 0.7428571428571429,
            "metric": "F1",
            "output": "A semicharacter RNN architecture is inspired by psycholinguistic studies and processes a sentence of words with misspelled characters, predicting the correct words at each step.",
            "ground_truth": "A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters",
            "peak_memory_mb": 16390.73583984375,
            "latency_ms": 5142.063353967387
        },
        {
            "index": 156,
            "score": 1.0,
            "metric": "F1",
            "output": "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish.",
            "ground_truth": "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish",
            "peak_memory_mb": 16069.6416015625,
            "latency_ms": 3803.8525659940206
        },
        {
            "index": 157,
            "score": 1.0,
            "metric": "F1",
            "output": "NCEL consistently outperforms various baselines with a favorable generalization ability.",
            "ground_truth": "NCEL consistently outperforms various baselines with a favorable generalization ability",
            "peak_memory_mb": 16390.734375,
            "latency_ms": 3280.9208689723164
        },
        {
            "index": 158,
            "score": 1.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "Yes",
            "peak_memory_mb": 16470.69677734375,
            "latency_ms": 2585.3069910081103
        },
        {
            "index": 159,
            "score": 0.0,
            "metric": "F1",
            "output": "The available training set.",
            "ground_truth": "error detection system by Rei2016",
            "peak_memory_mb": 15909.9697265625,
            "latency_ms": 1143.0335760232992
        },
        {
            "index": 160,
            "score": 0.36363636363636365,
            "metric": "F1",
            "output": "The 2010 i2b2/VA dataset.",
            "ground_truth": "clinical notes from the CE task in 2010 i2b2/VA",
            "peak_memory_mb": 16229.56396484375,
            "latency_ms": 2539.8591460543685
        },
        {
            "index": 161,
            "score": 0.4137931034482759,
            "metric": "F1",
            "output": "By masking words in the decoder, the refine process provides a more complete input sequence to BERT, which is consistent with its pre-training processes, allowing the decoder to generate more fluent and natural sequences.",
            "ground_truth": "ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences.",
            "peak_memory_mb": 16390.734375,
            "latency_ms": 5426.8199219950475
        },
        {
            "index": 162,
            "score": 0.17391304347826086,
            "metric": "F1",
            "output": "The article does not explicitly mention the dataset used, but it mentions various corpora such as the book corpus, PPDB (Paraphrase Database), and Twitter.",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 15909.96826171875,
            "latency_ms": 2905.307865003124
        },
        {
            "index": 163,
            "score": 0.0,
            "metric": "F1",
            "output": "Term Frequency-Inverse Document Frequency (TF-IDF) features.",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 15909.97021484375,
            "latency_ms": 1630.8899520081468
        },
        {
            "index": 164,
            "score": 0.5531914893617021,
            "metric": "F1",
            "output": "The dataset is annotated as no evidence of depression or evidence of depression with one or more depressive symptoms, further classified into subtypes such as depressed mood, disturbed sleep, or fatigue or loss of energy.",
            "ground_truth": "no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy",
            "peak_memory_mb": 15909.9697265625,
            "latency_ms": 3542.905548005365
        },
        {
            "index": 165,
            "score": 0.0,
            "metric": "F1",
            "output": "The eight publicly available NER tasks used in BIBREF2, which are not explicitly listed in the article, but are mentioned as being used in BIBREF2.",
            "ground_truth": "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800",
            "peak_memory_mb": 16309.6474609375,
            "latency_ms": 4608.514024002943
        },
        {
            "index": 166,
            "score": 0.7142857142857143,
            "metric": "F1",
            "output": "The training data was translated using the machine translation platform Apertium.",
            "ground_truth": "using the machine translation platform Apertium ",
            "peak_memory_mb": 15989.8046875,
            "latency_ms": 1851.2577340006828
        },
        {
            "index": 167,
            "score": 0.4210526315789474,
            "metric": "F1",
            "output": "multinomial Naive Bayes classifier.",
            "ground_truth": "AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier",
            "peak_memory_mb": 16309.64892578125,
            "latency_ms": 2522.818934987299
        },
        {
            "index": 168,
            "score": 0.7272727272727274,
            "metric": "F1",
            "output": "A very simple logistic regression classifier with default parameters, where the input instances were represented with a single feature: the length of the sentence, for the SLC task, and a simple random baseline that generates spans and selects one of the 18 techniques randomly for the FLC task.",
            "ground_truth": "The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.",
            "peak_memory_mb": 16149.47705078125,
            "latency_ms": 5962.2630319790915
        },
        {
            "index": 169,
            "score": 0.2105263157894737,
            "metric": "F1",
            "output": "They compare with a baseline model based on conditional random fields (CRF) and a rule-based locator.",
            "ground_truth": "They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.",
            "peak_memory_mb": 16069.64208984375,
            "latency_ms": 2652.619399013929
        },
        {
            "index": 170,
            "score": 0.40816326530612246,
            "metric": "F1",
            "output": "By training only on left-biased or right-biased outlets and testing on the entire set of sources, or excluding particular sources that outweigh the others in terms of samples.",
            "ground_truth": "By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains",
            "peak_memory_mb": 16550.5302734375,
            "latency_ms": 5674.585553992074
        },
        {
            "index": 171,
            "score": 0.8571428571428571,
            "metric": "F1",
            "output": "The ancient Chinese data comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.",
            "ground_truth": "ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era",
            "peak_memory_mb": 16309.6474609375,
            "latency_ms": 4474.748864013236
        },
        {
            "index": 172,
            "score": 1.0,
            "metric": "F1",
            "output": "English.",
            "ground_truth": "English",
            "peak_memory_mb": 15989.80419921875,
            "latency_ms": 1123.9155830116943
        },
        {
            "index": 173,
            "score": 0.1,
            "metric": "F1",
            "output": "The Chinese dataset used was not specified in the article.",
            "ground_truth": "Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)",
            "peak_memory_mb": 16069.64013671875,
            "latency_ms": 1868.7725830241106
        },
        {
            "index": 174,
            "score": 0.3333333333333333,
            "metric": "F1",
            "output": "At least 5 layers (maximum pooling, user vector embedding, topic vector embedding, fully connected network, and softmax)",
            "ground_truth": "eight layers",
            "peak_memory_mb": 16390.73583984375,
            "latency_ms": 4045.381766045466
        },
        {
            "index": 175,
            "score": 0.06896551724137932,
            "metric": "F1",
            "output": "The datasets used in this paper include Flickr tags, numerical features from structured environmental datasets, and categorical features from CORINE land cover classes and soil types.",
            "ground_truth": " the same datasets as BIBREF7",
            "peak_memory_mb": 16470.69482421875,
            "latency_ms": 4837.073095957749
        },
        {
            "index": 176,
            "score": 0.6666666666666666,
            "metric": "F1",
            "output": "NUBes-PHI and the MEDDOCAN corpus.",
            "ground_truth": "MEDDOCAN, NUBes-PHI",
            "peak_memory_mb": 16470.69482421875,
            "latency_ms": 3144.9332699994557
        },
        {
            "index": 177,
            "score": 0.0,
            "metric": "F1",
            "output": "Unigram features, stylistic patterns, patterns related to situational disparity, and hashtag interpretations.",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16229.56396484375,
            "latency_ms": 3039.4978269468993
        },
        {
            "index": 178,
            "score": 0.9333333333333333,
            "metric": "F1",
            "output": "Avg. MCC and avg. +ve F1 score.",
            "ground_truth": "Coverage, Avg. MCC and avg. +ve F1 score",
            "peak_memory_mb": 16790.1650390625,
            "latency_ms": 4562.385194003582
        },
        {
            "index": 179,
            "score": 1.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "Yes",
            "peak_memory_mb": 15983.4892578125,
            "latency_ms": 949.6930129826069
        },
        {
            "index": 180,
            "score": 0.8,
            "metric": "F1",
            "output": "Galatasaray and FenerbahÃ§e.",
            "ground_truth": "Galatasaray, FenerbahÃ§e",
            "peak_memory_mb": 15926.07275390625,
            "latency_ms": 1472.8591829771176
        },
        {
            "index": 181,
            "score": 0.37837837837837834,
            "metric": "F1",
            "output": "The article describes experiments on the transformation from non-ironic sentences to ironic sentences, as well as from ironic sentences to non-ironic sentences.",
            "ground_truth": "Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences",
            "peak_memory_mb": 16449.90625,
            "latency_ms": 4666.001226985827
        },
        {
            "index": 182,
            "score": 0.6666666666666667,
            "metric": "F1",
            "output": "It pays attention to adjacent characters of each position by casting the localness relationship between characters as a fixed Gaussian weight for attention based on the distance between characters.",
            "ground_truth": "pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters",
            "peak_memory_mb": 16309.64794921875,
            "latency_ms": 4219.048457976896
        },
        {
            "index": 183,
            "score": 0.28571428571428575,
            "metric": "F1",
            "output": "Facebook and Twitter.",
            "ground_truth": "Facebook status update messages",
            "peak_memory_mb": 16309.6484375,
            "latency_ms": 2144.940313999541
        },
        {
            "index": 184,
            "score": 0.8,
            "metric": "F1",
            "output": "Features extracted from the baseline CNN architecture.",
            "ground_truth": " The features extracted from CNN.",
            "peak_memory_mb": 16502.6875,
            "latency_ms": 3101.2012570281513
        },
        {
            "index": 185,
            "score": 0.5454545454545454,
            "metric": "F1",
            "output": "The number of clusters (k) was varied.",
            "ground_truth": "number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding",
            "peak_memory_mb": 16069.64013671875,
            "latency_ms": 1806.0307249543257
        },
        {
            "index": 186,
            "score": 0.0,
            "metric": "F1",
            "output": "Their system ranked second (EI-Reg, EI-Oc), fourth (V-Reg) and fifth (V-Oc) on the SemEval AIT-2018 leaderboard.",
            "ground_truth": "column Ens Test in Table TABREF19",
            "peak_memory_mb": 15989.8046875,
            "latency_ms": 3529.282203002367
        },
        {
            "index": 187,
            "score": 0.7000000000000001,
            "metric": "F1",
            "output": "It consists of 53 documents, with 8,275 sentences and 167,739 words in total.",
            "ground_truth": "8,275 sentences and 167,739 words in total",
            "peak_memory_mb": 16069.64013671875,
            "latency_ms": 2628.96840198664
        },
        {
            "index": 188,
            "score": 0.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "Unanswerable",
            "peak_memory_mb": 16104.291015625,
            "latency_ms": 1438.5649059549905
        },
        {
            "index": 189,
            "score": 0.39999999999999997,
            "metric": "F1",
            "output": "text categorization, sentiment classification.",
            "ground_truth": "text classification for themes including sentiment, web-page, science, medical and healthcare",
            "peak_memory_mb": 16309.6474609375,
            "latency_ms": 2296.614832011983
        },
        {
            "index": 190,
            "score": 0.0625,
            "metric": "F1",
            "output": "Previous methods include term frequency methods, as well as methods developed using TREC questions, such as CNN, LSTM, and BERT models, and other methods in the biomedical and education domains.",
            "ground_truth": "bag-of-words model, CNN",
            "peak_memory_mb": 16712.2041015625,
            "latency_ms": 6874.166499008425
        },
        {
            "index": 191,
            "score": 0.2,
            "metric": "F1",
            "output": "Much larger, specifically a few hundred million tokens, compared to 20 million tokens used in ELMoForManyLangs.",
            "ground_truth": "By 14 times.",
            "peak_memory_mb": 16229.56201171875,
            "latency_ms": 3490.369456994813
        },
        {
            "index": 192,
            "score": 0.4,
            "metric": "F1",
            "output": "The dataset contains 6946 sentences.",
            "ground_truth": "3606",
            "peak_memory_mb": 16149.47705078125,
            "latency_ms": 1960.017419012729
        },
        {
            "index": 193,
            "score": 0.6666666666666666,
            "metric": "F1",
            "output": "Eusboost, MWMOTE, and MLP.",
            "ground_truth": "MLP",
            "peak_memory_mb": 16149.4755859375,
            "latency_ms": 2083.2032510079443
        },
        {
            "index": 194,
            "score": 1.0,
            "metric": "F1",
            "output": "Yes.",
            "ground_truth": "Yes",
            "peak_memory_mb": 16309.650390625,
            "latency_ms": 2099.346028000582
        },
        {
            "index": 195,
            "score": 0.0,
            "metric": "F1",
            "output": "No. They use the Wall Street Journal (WSJ) portion of the Penn Treebank, which is a dataset in English, but also mention training on a one billion word language modeling benchmark dataset, which may include other languages.",
            "ground_truth": "Yes",
            "peak_memory_mb": 16390.7353515625,
            "latency_ms": 6110.402902995702
        },
        {
            "index": 196,
            "score": 0.16666666666666669,
            "metric": "F1",
            "output": "0.6103 in one of the test batches for Factoid Question Answering task.",
            "ground_truth": "0.5115",
            "peak_memory_mb": 16974.55908203125,
            "latency_ms": 5915.155543014407
        },
        {
            "index": 197,
            "score": 0.5833333333333334,
            "metric": "F1",
            "output": "The Wall Street Journal (WSJ) portion of the Penn Treebank, specifically sections 02-21 for training and section 23 for evaluation.",
            "ground_truth": " Wall Street Journal (WSJ) portion of the Penn Treebank",
            "peak_memory_mb": 16390.7353515625,
            "latency_ms": 4574.216279957909
        },
        {
            "index": 198,
            "score": 0.0,
            "metric": "F1",
            "output": "They mention that general-purpose deep learning frameworks like TensorFlow, PyTorch, and Keras offer huge flexibility in DNN model design, but also require a large overhead of mastering these framework details.",
            "ground_truth": "By conducting a survey among engineers",
            "peak_memory_mb": 15889.64306640625,
            "latency_ms": 3238.694289000705
        },
        {
            "index": 199,
            "score": 0.16666666666666669,
            "metric": "F1",
            "output": "They achieve the state of the art on both SimpleQuestions and WebQSP, with SimpleQuestions having a significant performance boost when they use the top-3 relation detectors.",
            "ground_truth": "SimpleQuestions, WebQSP",
            "peak_memory_mb": 16550.53125,
            "latency_ms": 5641.420604020823
        }
    ]
}