{
    "task": "qasper",
    "version": "v1",
    "args": {
        "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "bench_version": "v1",
        "task_type": "single-doc",
        "num_samples": 10,
        "output_len": 64
    },
    "avg_score": 0.4592289845231021,
    "max_peak_memory_mb": 16887.47900390625,
    "details": [
        {
            "index": 0,
            "score": 0.2941176470588235,
            "metric": "F1",
            "output": "The ground truth for fake news is established by a single person manually annotating the tweets as containing fake news or not, based on categories described by Rubin et al.",
            "ground_truth": "Ground truth is not established in the paper",
            "peak_memory_mb": 16292.34716796875,
            "latency_ms": 1618.3665171265602
        },
        {
            "index": 1,
            "score": 0.5185185185185185,
            "metric": "F1",
            "output": "An extension of the NetVLAD approach that adds Ghost clusters to map noisy or irrelevant content into ghost clusters.",
            "ground_truth": "extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters",
            "peak_memory_mb": 16128.255859375,
            "latency_ms": 1186.5979540161788
        },
        {
            "index": 2,
            "score": 0.0,
            "metric": "F1",
            "output": "68.8% to 71.8%",
            "ground_truth": "the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)",
            "peak_memory_mb": 16375.3056640625,
            "latency_ms": 1025.4233891610056
        },
        {
            "index": 3,
            "score": 0.2,
            "metric": "F1",
            "output": "Context tweets, character-level features, and latent topic clustering (LTC) are proposed as additional features and context.",
            "ground_truth": "using tweets that one has replied or quoted to as contextual information",
            "peak_memory_mb": 16047.0576171875,
            "latency_ms": 1145.513336872682
        },
        {
            "index": 4,
            "score": 1.0,
            "metric": "F1",
            "output": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.",
            "ground_truth": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney",
            "peak_memory_mb": 16394.12841796875,
            "latency_ms": 2045.255510136485
        },
        {
            "index": 5,
            "score": 1.0,
            "metric": "F1",
            "output": "yes",
            "ground_truth": "Yes",
            "peak_memory_mb": 16617.8974609375,
            "latency_ms": 946.5995491482317
        },
        {
            "index": 6,
            "score": 0.0,
            "metric": "F1",
            "output": "Extrinsic evaluation, specifically using summary evaluation, is proposed for this task.",
            "ground_truth": "Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2",
            "peak_memory_mb": 16686.0673828125,
            "latency_ms": 1413.0260308738798
        },
        {
            "index": 7,
            "score": 0.9,
            "metric": "F1",
            "output": "The CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum.",
            "ground_truth": "CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum",
            "peak_memory_mb": 16887.47900390625,
            "latency_ms": 1832.8365599736571
        },
        {
            "index": 8,
            "score": 0.6190476190476191,
            "metric": "F1",
            "output": "The proposed approach, GM_KL, achieves better correlation than existing approaches for various metrics on the SCWS dataset and also performs better than other approaches on the benchmark word similarity and entailment datasets.",
            "ground_truth": "GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.",
            "peak_memory_mb": 16170.3125,
            "latency_ms": 1701.2717069592327
        },
        {
            "index": 9,
            "score": 0.060606060606060615,
            "metric": "F1",
            "output": "Their ensemble method works by selecting the best performing models according to validation performance and gradually adding them to the ensemble, discarding any that do not improve performance.",
            "ground_truth": "simply averaging the predictions from the constituent single models",
            "peak_memory_mb": 16605.09521484375,
            "latency_ms": 1803.139426978305
        }
    ]
}