{
    "args": {
        "model_id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "task": "narrativeqa",
        "num_samples": 5,
        "output_len": 64
    },
    "config": {
        "vocab_size": 128256,
        "max_position_embeddings": 131072,
        "hidden_size": 4096,
        "intermediate_size": 14336,
        "num_hidden_layers": 32,
        "num_attention_heads": 32,
        "num_key_value_heads": 8,
        "hidden_act": "silu",
        "initializer_range": 0.02,
        "rms_norm_eps": 1e-05,
        "pretraining_tp": 1,
        "use_cache": true,
        "rope_theta": 500000.0,
        "rope_scaling": {
            "factor": 8.0,
            "low_freq_factor": 1.0,
            "high_freq_factor": 4.0,
            "original_max_position_embeddings": 8192,
            "rope_type": "llama3"
        },
        "attention_bias": false,
        "attention_dropout": 0.0,
        "mlp_bias": false,
        "head_dim": 128,
        "return_dict": true,
        "output_hidden_states": false,
        "torchscript": false,
        "dtype": "float16",
        "pruned_heads": {},
        "tie_word_embeddings": false,
        "chunk_size_feed_forward": 0,
        "is_encoder_decoder": false,
        "is_decoder": false,
        "cross_attention_hidden_size": null,
        "add_cross_attention": false,
        "tie_encoder_decoder": false,
        "architectures": [
            "LlamaForCausalLM"
        ],
        "finetuning_task": null,
        "id2label": {
            "0": "LABEL_0",
            "1": "LABEL_1"
        },
        "label2id": {
            "LABEL_0": 0,
            "LABEL_1": 1
        },
        "task_specific_params": null,
        "problem_type": null,
        "tokenizer_class": null,
        "prefix": null,
        "bos_token_id": 128000,
        "pad_token_id": 128009,
        "eos_token_id": [
            128001,
            128008,
            128009
        ],
        "sep_token_id": null,
        "decoder_start_token_id": null,
        "max_length": 20,
        "min_length": 0,
        "do_sample": false,
        "early_stopping": false,
        "num_beams": 1,
        "temperature": 1.0,
        "top_k": 50,
        "top_p": 1.0,
        "typical_p": 1.0,
        "repetition_penalty": 1.0,
        "length_penalty": 1.0,
        "no_repeat_ngram_size": 0,
        "encoder_no_repeat_ngram_size": 0,
        "bad_words_ids": null,
        "num_return_sequences": 1,
        "output_scores": false,
        "return_dict_in_generate": false,
        "forced_bos_token_id": null,
        "forced_eos_token_id": null,
        "remove_invalid_values": false,
        "exponential_decay_length_penalty": null,
        "suppress_tokens": null,
        "begin_suppress_tokens": null,
        "num_beam_groups": 1,
        "diversity_penalty": 0.0,
        "_name_or_path": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "transformers_version": "4.57.3",
        "model_type": "llama",
        "tf_legacy_loss": false,
        "use_bfloat16": false,
        "output_attentions": false
    },
    "avg_f1": 0.18349276915545323,
    "max_peak_memory_mb": 24162.17724609375,
    "details": [
        {
            "index": 0,
            "input_length": 22692,
            "actual_input_len": 31299,
            "f1": 0.2857142857142857,
            "peak_memory_mb": 22803.83154296875,
            "latency_ms": 8061.283680144697,
            "throughput": 7.939182212087991,
            "output": " Saltram is living at Wimbledon with the Mulvilles.",
            "ground_truth": "He is a guest in the home of the Mulvilles."
        },
        {
            "index": 1,
            "input_length": 23316,
            "actual_input_len": 31346,
            "f1": 0.10169491525423728,
            "peak_memory_mb": 22817.37646484375,
            "latency_ms": 10267.80452625826,
            "throughput": 6.233075419027533,
            "output": " Ann's heart is entirely engrossed by one object, and she does not feel the same kind of affection for Mary. Her memory often reverts to past scenes, and she has unavailing wishes that make time seem to loiter. She cannot bear to obtrude her affection on Mary or receive love as",
            "ground_truth": "Because she is in unrequited love with someone else"
        },
        {
            "index": 2,
            "input_length": 5397,
            "actual_input_len": 7977,
            "f1": 0.22222222222222224,
            "peak_memory_mb": 17229.84521484375,
            "latency_ms": 1763.3438697084785,
            "throughput": 36.294679160100905,
            "output": " Atlas' mountain, within a cavern, by a secret fountain.",
            "ground_truth": "The Atlas Mountains"
        },
        {
            "index": 3,
            "input_length": 6592,
            "actual_input_len": 8978,
            "f1": 0.19672131147540983,
            "peak_memory_mb": 17472.541015625,
            "latency_ms": 3571.0007399320602,
            "throughput": 17.922146944505428,
            "output": " Crito came to persuade Socrates to escape from prison, as he believed that Socrates' death would bring him shame and that people would think he was willing to sacrifice Socrates for money. Crito was concerned about the opinion of the many and the potential consequences of Socrates' death on his friends and family.",
            "ground_truth": "To smuggle Socrates out of prison and into a life of exile."
        },
        {
            "index": 4,
            "input_length": 27745,
            "actual_input_len": 36989,
            "f1": 0.1111111111111111,
            "peak_memory_mb": 24162.17724609375,
            "latency_ms": 12593.934446573257,
            "throughput": 5.08181142847016,
            "output": " The text doesn't explicitly state why Baron Henry didn't kill Otto, but it can be inferred that he wanted to humiliate and terrorize the young Baron instead of simply killing him. By cutting off Otto's hand, Baron Henry was able to inflict a painful and debilitating injury that would leave Otto a cripple for the",
            "ground_truth": "Otto was so young."
        }
    ]
}